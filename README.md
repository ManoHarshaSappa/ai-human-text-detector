# AI vs Human Text Detector using BERT

This project aims to classify whether a given piece of text is written by a human or generated by an AI model. The classification is done using a fine-tuned BERT (Bidirectional Encoder Representations from Transformers) model. The goal is to support content authenticity checking, especially as generative AI tools are increasingly being used in everyday writing.

## Objective
The main objective of this project is to build a text classifier that can automatically detect the origin of a text â€” whether it has been written by a human or generated by an AI. The model is trained using a balanced dataset and evaluated for accuracy and overall performance. This project also explores basic preprocessing and fine-tuning techniques using Hugging Face Transformers.

## Dataset

The dataset was originally downloaded from Kaggle:  
Link: https://www.kaggle.com/datasets/shanegerami/ai-vs-human-text

Due to the size limit on GitHub, the dataset is not uploaded here. If you want to use the same data, please download it manually from the link provided above.

The original dataset contains:
- Around 310,000 human-written texts
- Around 180,000 AI-generated texts

To keep the training fair and balanced, a subset was created:
- 5,000 samples each of human and AI-written texts
- Total: 10,000 samples
- 
## Preprocessing

Before training, the text data is preprocessed using the following steps:
- Lowercasing all text
- Removing punctuation and stopwords
- Applying stemming using NLTK
- Tokenization using `BertTokenizer` from Hugging Face
- All sequences were padded/truncated to a maximum length of 246 tokens

## Train-Test Split

The dataset was split into three parts:
- 70% for training
- 15% for validation
- 15% for testing

This helps in evaluating the model properly and preventing overfitting.


## Model Details

The model used is `bert-base-uncased` from Hugging Face. It has been fine-tuned for binary classification (AI vs Human). Details of the architecture and training are as follows:

- Model: BERT with classification head (`num_labels=2`)
- Loss Function: CrossEntropyLoss
- Optimizer: AdamW with learning rate of 1e-5
- Batch Size: 10 (chosen for compatibility with Apple M1/M2 using MPS)
- Number of Epochs: 3

The fine-tuned model and tokenizer are saved to the directory: `saved_bert_model/`


## Evaluation Results

The model was evaluated on the test set. Below are the key performance metrics:

Test Accuracy: 91.73%
